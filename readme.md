----
Android Skip Ads Yolov5 Project
----
---

![yolov5](atricle_images/splash.jpg)  

[YOLOv5](https://github.com/ultralytics/yolov5) ğŸš€ is a family of object detection architectures and models pretrained on the COCO dataset, and represents Ultralytics open-source research into future vision AI methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development.

**1. ç¯å¢ƒæ­å»º**  
è‡ªåŠ¨å¹¿å‘Šè·³è¿‡è¿™ä¸ªæƒ³æ³•,ä¸è¿‡ç”±äºå„ç§åŸå› ä¸€ç›´æ²¡æœ‰å®æ—¶ã€‚çŸ¥é“æœ€è¿‘æ‰åˆé‡æ–°å¼€å§‹æŠ˜è…¾è¿™ä¸ªä¸œè¥¿,yolov5çš„å®‰è£…è¿™é‡Œå°±ä¸å†è¯´æ˜äº†ï¼Œå»ºè®®ä½¿ç”¨anacondaå®‰è£…ã€‚æˆ‘åˆ›å»ºäº†ä¸€ä¸ªcondaç¯å¢ƒå¯ä»¥ç›´æ¥ä¸‹è½½ä¹‹åé€šè¿‡condaå®‰è£…ï¼š  
```shell
# 1. conda ç¯å¢ƒåœ°å€ï¼šhttps://anaconda.org/obaby/yolov5
# 2. ä¸‹è½½å¯¹åº”æ“ä½œç³»ç»Ÿçš„ç¯å¢ƒymlè„šæœ¬
# 3. åœ¨Terminal æˆ–è€… an Anaconda Promptå†…æ‰§è¡Œ
conda env create user/my-environment
source activate my-environment
```  
![conda](https://anaconda.org/obaby/yolov5/badges/version.svg)
å¦‚æœä¸æƒ³ä½¿ç”¨ä¸Šé¢çš„å®‰è£…æ–¹æ³•å¯ä»¥å‚è€ƒï¼šhttps://blog.csdn.net/oJiWuXuan/article/details/107558286 å’Œhttps://github.com/ultralytics/yolov5 æŒ‰ç…§å®˜æ–¹æŒ‡å¯¼è¿›è¡Œå®‰è£…ã€‚  

**2.æ•°æ®å‡†å¤‡**  
å°†æ‰‹æœºä¸Šå·²ç»å‡†å¤‡å¥½çš„æˆªå›¾å¤åˆ¶åˆ°Screenshotç›®å½•ä¸‹ï¼Œä½¿ç”¨[labelImg](https://github.com/tzutalin/labelImg){conda ç¯å¢ƒåœ°å€ï¼šhttps://anaconda.org/obaby/labelimg å¯ä»¥ç›´æ¥å¯¼å…¥ä½¿ç”¨},å¯¹å›¾ç‰‡è¿›è¡Œæ ‡è®°ã€‚å°†æ ‡è®°åçš„xmlæ–‡ä»¶ä¿å­˜åˆ°xmlsç›®å½•ä¸‹ã€‚    
å±å¹•æˆªå›¾ï¼š  
![screnshots](atricle_images/screenshots.jpg)  
labelimgæ ‡è®°ï¼š  
![labelimage](atricle_images/labelimg.jpg)  
xmlæ–‡ä»¶è·¯å¾„ï¼š  
![xmls](atricle_images/xmls.jpg)  
**3.æ„å»ºæ•°æ®é›†ï¼š**  
åœ¨æ ¹ç›®å½•ä¸‹åˆ›å»ºmake_text.pyä»£ç å¦‚ä¸‹(**ä»¥ä¸‹ä»£ç åŸºæœ¬éƒ½æ˜¯æ‹·è´è‡ªï¼šhttps://blog.csdn.net/oJiWuXuan/article/details/107558286 è¿™ç¯‡æ–‡ç« ï¼Œè¡¨ç¤ºæ„Ÿè°¢ï¼ï¼**)ï¼š  
```python
import os
import random

# https://blog.csdn.net/oJiWuXuan/article/details/107558286

trainval_percent = 0.9
train_percent = 0.9
xmlfilepath = 'xmls'
txtsavepath = 'Screenshots'
total_xml = os.listdir(xmlfilepath)

num = len(total_xml)
list = range(num)
tv = int(num * trainval_percent)
tr = int(tv * train_percent)
trainval = random.sample(list, tv)
train = random.sample(trainval, tr)

ftrainval = open('data/ImageSets/trainval.txt', 'w')
ftest = open('data/ImageSets/test.txt', 'w')
ftrain = open('data/ImageSets/train.txt', 'w')
fval = open('data/ImageSets/val.txt', 'w')

for i in list:
    name = total_xml[i][:-4] + '\n'
    if i in trainval:
        ftrainval.write(name)
        if i in train:
            ftrain.write(name)
        else:
            fval.write(name)
    else:
        ftest.write(name)

ftrainval.close()
ftrain.close()
fval.close()
ftest.close()

```  
åˆ›å»ºvoc_label.pyä»£ç å¦‚ä¸‹ï¼Œéœ€è¦æ³¨æ„classesä¸­çš„åˆ—åˆ«ä¿¡æ¯ï¼š  
```python
# xmlè§£æåŒ…
import xml.etree.ElementTree as ET
import pickle
import os
# os.listdir() æ–¹æ³•ç”¨äºè¿”å›æŒ‡å®šçš„æ–‡ä»¶å¤¹åŒ…å«çš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹çš„åå­—çš„åˆ—è¡¨
from os import listdir, getcwd
from os.path import join


sets = ['train', 'test', 'val']
classes = ['skip']


# è¿›è¡Œå½’ä¸€åŒ–æ“ä½œ
def convert(size, box): # size:(åŸå›¾w,åŸå›¾h) , box:(xmin,xmax,ymin,ymax)
    dw = 1./size[0]     # 1/w
    dh = 1./size[1]     # 1/h
    x = (box[0] + box[1])/2.0   # ç‰©ä½“åœ¨å›¾ä¸­çš„ä¸­å¿ƒç‚¹xåæ ‡
    y = (box[2] + box[3])/2.0   # ç‰©ä½“åœ¨å›¾ä¸­çš„ä¸­å¿ƒç‚¹yåæ ‡
    w = box[1] - box[0]         # ç‰©ä½“å®é™…åƒç´ å®½åº¦
    h = box[3] - box[2]         # ç‰©ä½“å®é™…åƒç´ é«˜åº¦
    x = x*dw    # ç‰©ä½“ä¸­å¿ƒç‚¹xçš„åæ ‡æ¯”(ç›¸å½“äº x/åŸå›¾w)
    w = w*dw    # ç‰©ä½“å®½åº¦çš„å®½åº¦æ¯”(ç›¸å½“äº w/åŸå›¾w)
    y = y*dh    # ç‰©ä½“ä¸­å¿ƒç‚¹yçš„åæ ‡æ¯”(ç›¸å½“äº y/åŸå›¾h)
    h = h*dh    # ç‰©ä½“å®½åº¦çš„å®½åº¦æ¯”(ç›¸å½“äº h/åŸå›¾h)
    return (x, y, w, h)    # è¿”å› ç›¸å¯¹äºåŸå›¾çš„ç‰©ä½“ä¸­å¿ƒç‚¹çš„xåæ ‡æ¯”,yåæ ‡æ¯”,å®½åº¦æ¯”,é«˜åº¦æ¯”,å–å€¼èŒƒå›´[0-1]


# year ='2012', å¯¹åº”å›¾ç‰‡çš„idï¼ˆæ–‡ä»¶åï¼‰
def convert_annotation(image_id):
    '''
    å°†å¯¹åº”æ–‡ä»¶åçš„xmlæ–‡ä»¶è½¬åŒ–ä¸ºlabelæ–‡ä»¶ï¼Œxmlæ–‡ä»¶åŒ…å«äº†å¯¹åº”çš„bundingæ¡†ä»¥åŠå›¾ç‰‡é•¿æ¬¾å¤§å°ç­‰ä¿¡æ¯ï¼Œ
    é€šè¿‡å¯¹å…¶è§£æï¼Œç„¶åè¿›è¡Œå½’ä¸€åŒ–æœ€ç»ˆè¯»åˆ°labelæ–‡ä»¶ä¸­å»ï¼Œä¹Ÿå°±æ˜¯è¯´
    ä¸€å¼ å›¾ç‰‡æ–‡ä»¶å¯¹åº”ä¸€ä¸ªxmlæ–‡ä»¶ï¼Œç„¶åé€šè¿‡è§£æå’Œå½’ä¸€åŒ–ï¼Œèƒ½å¤Ÿå°†å¯¹åº”çš„ä¿¡æ¯ä¿å­˜åˆ°å”¯ä¸€ä¸€ä¸ªlabelæ–‡ä»¶ä¸­å»
    labalæ–‡ä»¶ä¸­çš„æ ¼å¼ï¼šcalss x y w hã€€ã€€åŒæ—¶ï¼Œä¸€å¼ å›¾ç‰‡å¯¹åº”çš„ç±»åˆ«æœ‰å¤šä¸ªï¼Œæ‰€ä»¥å¯¹åº”çš„ï½‚ï½•ï½ï½„ï½‰ï½ï½‡çš„ä¿¡æ¯ä¹Ÿæœ‰å¤šä¸ª
    '''
    # å¯¹åº”çš„é€šè¿‡year æ‰¾åˆ°ç›¸åº”çš„æ–‡ä»¶å¤¹ï¼Œå¹¶ä¸”æ‰“å¼€ç›¸åº”image_idçš„xmlæ–‡ä»¶ï¼Œå…¶å¯¹åº”bundæ–‡ä»¶
    in_file = open('train/%s.xml' % (image_id), encoding='utf-8')
    # å‡†å¤‡åœ¨å¯¹åº”çš„image_id ä¸­å†™å…¥å¯¹åº”çš„labelï¼Œåˆ†åˆ«ä¸º
    # <object-class> <x> <y> <width> <height>
    out_file = open('data/labels/%s.txt' % (image_id), 'w', encoding='utf-8')
    # è§£æxmlæ–‡ä»¶
    tree = ET.parse(in_file)
    # è·å¾—å¯¹åº”çš„é”®å€¼å¯¹
    root = tree.getroot()
    # è·å¾—å›¾ç‰‡çš„å°ºå¯¸å¤§å°
    size = root.find('size')
    # å¦‚æœxmlå†…çš„æ ‡è®°ä¸ºç©ºï¼Œå¢åŠ åˆ¤æ–­æ¡ä»¶
    if size != None:
        # è·å¾—å®½
        w = int(size.find('width').text)
        # è·å¾—é«˜
        h = int(size.find('height').text)
        # éå†ç›®æ ‡obj
        for obj in root.iter('object'):
            # è·å¾—difficult ï¼Ÿï¼Ÿ
            difficult = obj.find('difficult').text
            # è·å¾—ç±»åˆ« =string ç±»å‹
            cls = obj.find('name').text
            # å¦‚æœç±»åˆ«ä¸æ˜¯å¯¹åº”åœ¨æˆ‘ä»¬é¢„å®šå¥½çš„classæ–‡ä»¶ä¸­ï¼Œæˆ–difficult==1åˆ™è·³è¿‡
            if cls not in classes or int(difficult) == 1:
                continue
            # é€šè¿‡ç±»åˆ«åç§°æ‰¾åˆ°id
            cls_id = classes.index(cls)
            # æ‰¾åˆ°bndbox å¯¹è±¡
            xmlbox = obj.find('bndbox')
            # è·å–å¯¹åº”çš„bndboxçš„æ•°ç»„ = ['xmin','xmax','ymin','ymax']
            b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text),
                 float(xmlbox.find('ymax').text))
            print(image_id, cls, b)
            # å¸¦å…¥è¿›è¡Œå½’ä¸€åŒ–æ“ä½œ
            # w = å®½, h = é«˜ï¼Œ b= bndboxçš„æ•°ç»„ = ['xmin','xmax','ymin','ymax']
            bb = convert((w, h), b)
            # bb å¯¹åº”çš„æ˜¯å½’ä¸€åŒ–åçš„(x,y,w,h)
            # ç”Ÿæˆ calss x y w h åœ¨labelæ–‡ä»¶ä¸­
            out_file.write(str(cls_id) + " " + " ".join([str(a) for a in bb]) + '\n')


# è¿”å›å½“å‰å·¥ä½œç›®å½•
wd = getcwd()
print(wd)


for image_set in sets:
    '''
    å¯¹æ‰€æœ‰çš„æ–‡ä»¶æ•°æ®é›†è¿›è¡Œéå†
    åšäº†ä¸¤ä¸ªå·¥ä½œï¼š
ã€€ã€€ã€€ã€€ï¼‘ï¼å°†æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶éƒ½éå†ä¸€éï¼Œå¹¶ä¸”å°†å…¶æ‰€æœ‰çš„å…¨è·¯å¾„éƒ½å†™åœ¨å¯¹åº”çš„txtæ–‡ä»¶ä¸­å»ï¼Œæ–¹ä¾¿å®šä½
ã€€ã€€ã€€ã€€ï¼’ï¼åŒæ—¶å¯¹æ‰€æœ‰çš„å›¾ç‰‡æ–‡ä»¶è¿›è¡Œè§£æå’Œè½¬åŒ–ï¼Œå°†å…¶å¯¹åº”çš„bundingbox ä»¥åŠç±»åˆ«çš„ä¿¡æ¯å…¨éƒ¨è§£æå†™åˆ°label æ–‡ä»¶ä¸­å»
    ã€€ã€€ã€€ã€€ã€€æœ€åå†é€šè¿‡ç›´æ¥è¯»å–æ–‡ä»¶ï¼Œå°±èƒ½æ‰¾åˆ°å¯¹åº”çš„label ä¿¡æ¯
    '''
    # å…ˆæ‰¾labelsæ–‡ä»¶å¤¹å¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
    if not os.path.exists('data/labels/'):
        os.makedirs('data/labels/')
    # è¯»å–åœ¨ImageSets/Main ä¸­çš„trainã€test..ç­‰æ–‡ä»¶çš„å†…å®¹
    # åŒ…å«å¯¹åº”çš„æ–‡ä»¶åç§°
    image_ids = open('data/ImageSets/%s.txt' % (image_set)).read().strip().split()
    # æ‰“å¼€å¯¹åº”çš„2012_train.txt æ–‡ä»¶å¯¹å…¶è¿›è¡Œå†™å…¥å‡†å¤‡
    list_file = open('data/%s.txt' % (image_set), 'w')
    # å°†å¯¹åº”çš„æ–‡ä»¶_idä»¥åŠå…¨è·¯å¾„å†™è¿›å»å¹¶æ¢è¡Œ
    for image_id in image_ids:
        list_file.write('data/images/%s.jpg\n' % (image_id))
        # è°ƒç”¨  year = å¹´ä»½  image_id = å¯¹åº”çš„æ–‡ä»¶å_id
        convert_annotation(image_id)
    # å…³é—­æ–‡ä»¶
    list_file.close()

# os.system(â€˜comandâ€™) ä¼šæ‰§è¡Œæ‹¬å·ä¸­çš„å‘½ä»¤ï¼Œå¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œï¼Œè¿™æ¡è¯­å¥è¿”å›0ï¼Œå¦åˆ™è¿”å›1
# os.system("cat 2007_train.txt 2007_val.txt 2012_train.txt 2012_val.txt > train.txt")
# os.system("cat 2007_train.txt 2007_val.txt 2007_test.txt 2012_train.txt 2012_val.txt > train.all.txt")

```
åˆ†åˆ«è¿è¡ŒmakeTxt.pyå’Œvoc_label.pyã€‚

make_text.pyä¸»è¦æ˜¯å°†æ•°æ®é›†åˆ†ç±»æˆè®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†ï¼Œé»˜è®¤trainï¼Œvalï¼ŒtestæŒ‰ç…§8ï¼š1ï¼š1çš„æ¯”ä¾‹è¿›è¡Œéšæœºåˆ†ç±»ï¼Œè¿è¡ŒåImagesSetsæ–‡ä»¶å¤¹ä¸­ä¼šå‡ºç°å››ä¸ªæ–‡ä»¶ï¼Œä¸»è¦æ˜¯ç”Ÿæˆçš„è®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†çš„å›¾ç‰‡åç§°ï¼Œå¦‚ä¸‹å›¾ã€‚åŒæ—¶dataç›®å½•ä¸‹ä¹Ÿä¼šå‡ºç°è¿™å››ä¸ªæ–‡ä»¶ï¼Œå†…å®¹æ˜¯è®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†çš„å›¾ç‰‡è·¯å¾„ã€‚
![txt](atricle_images/txt.jpg)  
voc_label.pyä¸»è¦æ˜¯å°†å›¾ç‰‡æ•°æ®é›†æ ‡æ³¨åçš„xmlæ–‡ä»¶ä¸­çš„æ ‡æ³¨ä¿¡æ¯è¯»å–å‡ºæ¥å¹¶å†™å…¥txtæ–‡ä»¶ï¼Œè¿è¡Œååœ¨labelsæ–‡ä»¶å¤¹ä¸­å‡ºç°æ‰€æœ‰å›¾ç‰‡æ•°æ®é›†çš„æ ‡æ³¨ä¿¡æ¯ï¼Œå¦‚ä¸‹å›¾ï¼š  
![label](atricle_images/label.jpg)  
åˆ°è¿™é‡Œï¼Œæ•°æ®èµ„æºå‡†å¤‡å°±okäº†ã€‚
**4.ä¿®æ”¹é…ç½®æ–‡ä»¶**  
åœ¨dataç›®å½•ä¸‹åˆ›å»ºads.ymlå†…å®¹å¦‚ä¸‹ï¼š  
```yaml
# COCO 2017 dataset http://cocodataset.org
# Download command: bash yolov5/data/get_coco2017.sh
# Train command: python train.py --data ./data/coco.yaml
# Dataset should be placed next to yolov5 folder:
#   /parent_folder
#     /coco
#     /yolov5


# train and val datasets (image directory or *.txt file with image paths)
train: data/train.txt  # 118k images
val: data/val.txt  # 5k images
test: data/test.txt  # 20k images for submission to https://competitions.codalab.org/competitions/20794

# number of classes
nc: 1

# class names
names: ['skip']

# Print classes
# with open('data/coco.yaml') as f:
#   d = yaml.load(f, Loader=yaml.FullLoader)  # dict
#   for i, x in enumerate(d['names']):
#     print(i, x)

```  
ä¿®æ”¹ç½‘ç»œå‚æ•°models/yolov5s.yaml(è¿™é‡Œå–å†³äºä½ ä½¿ç”¨äº†å“ªä¸ªæ¨¡å‹å°±å»ä¿®æ”¹å¯¹äºçš„æ–‡ä»¶ï¼Œè¯¥é¡¹ç›®ä¸­ä½¿ç”¨çš„æ˜¯yolov5sæ¨¡å‹),ä¸»è¦ä¿®æ”¹ncå‚æ•°ï¼š
```yaml
# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license

# Parameters
nc: 1  # number of classes
depth_multiple: 0.33  # model depth multiple
width_multiple: 0.50  # layer channel multiple
anchors:
  - [10,13, 16,30, 33,23]  # P3/8
  - [30,61, 62,45, 59,119]  # P4/16
  - [116,90, 156,198, 373,326]  # P5/32

# YOLOv5 backbone
backbone:
  # [from, number, module, args]
  [[-1, 1, Focus, [64, 3]],  # 0-P1/2
   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4
   [-1, 3, C3, [128]],
   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8
   [-1, 9, C3, [256]],
   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16
   [-1, 9, C3, [512]],
   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32
   [-1, 1, SPP, [1024, [5, 9, 13]]],
   [-1, 3, C3, [1024, False]],  # 9
  ]

# YOLOv5 head
head:
  [[-1, 1, Conv, [512, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 6], 1, Concat, [1]],  # cat backbone P4
   [-1, 3, C3, [512, False]],  # 13

   [-1, 1, Conv, [256, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 4], 1, Concat, [1]],  # cat backbone P3
   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)

   [-1, 1, Conv, [256, 3, 2]],
   [[-1, 14], 1, Concat, [1]],  # cat head P4
   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)

   [-1, 1, Conv, [512, 3, 2]],
   [[-1, 10], 1, Concat, [1]],  # cat head P5
   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)

   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)
  ]
```  
train_ads.py å‚æ•°ä¿®æ”¹ï¼Œä¸»è¦å‚æ•°è§£é‡Šå¦‚ä¸‹ã€‚æˆ‘ä»¬å¹³æ—¶è®­ç»ƒçš„è¯ï¼Œä¸»è¦ç”¨åˆ°çš„åªæœ‰è¿™å‡ ä¸ªå‚æ•°è€Œå·²ï¼šâ€“weightsï¼Œâ€“cfgï¼Œâ€“dataï¼Œâ€“epochsï¼Œâ€“batch-sizeï¼Œâ€“img-sizeï¼Œâ€“projectï¼š  
```yaml
def parse_opt(known=False):
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', type=str, default='yolov5s.pt', help='initial weights path')
    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')
    parser.add_argument('--data', type=str, default='data/ads.yaml', help='dataset.yaml path')
    parser.add_argument('--hyp', type=str, default='data/hyps/hyp.scratch.yaml', help='hyperparameters path')
    parser.add_argument('--epochs', type=int, default=300)
    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs')
    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')
    parser.add_argument('--rect', action='store_true', help='rectangular training')
    parser.add_argument('--resume', nargs='?', const=True, default=False, help='resume most recent training')
    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')
    parser.add_argument('--noval', action='store_true', help='only validate final epoch')
    parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')
    parser.add_argument('--evolve', type=int, nargs='?', const=300, help='evolve hyperparameters for x generations')
    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')
    parser.add_argument('--cache', type=str, nargs='?', const='ram', help='--cache images in "ram" (default) or "disk"')
    parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')
    parser.add_argument('--single-cls', default=True,action='store_true', help='train multi-class data as single-class')
    parser.add_argument('--adam', action='store_true', help='use torch.optim.Adam() optimizer')
    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')
    parser.add_argument('--workers', type=int, default=8, help='maximum number of dataloader workers')
    parser.add_argument('--project', default='runs/train', help='save to project/name')
    parser.add_argument('--entity', default=None, help='W&B entity')
    parser.add_argument('--name', default='exp', help='save to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--quad', action='store_true', help='quad dataloader')
    parser.add_argument('--linear-lr', action='store_true', help='linear LR')
    parser.add_argument('--label-smoothing', type=float, default=0.0, help='Label smoothing epsilon')
    parser.add_argument('--upload_dataset', action='store_true', help='Upload dataset as W&B artifact table')
    parser.add_argument('--bbox_interval', type=int, default=-1, help='Set bounding-box image logging interval for W&B')
    parser.add_argument('--save_period', type=int, default=-1, help='Log model after every "save_period" epoch')
    parser.add_argument('--artifact_alias', type=str, default="latest", help='version of dataset artifact to be used')
    parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')
    parser.add_argument('--freeze', type=int, default=0, help='Number of layers to freeze. backbone=10, all=24')
    parser.add_argument('--patience', type=int, default=30, help='EarlyStopping patience (epochs)')
    opt = parser.parse_known_args()[0] if known else parser.parse_args()
    return opt
```
**5.è®­ç»ƒæ¨¡å‹**  
ç¯å¢ƒæ­å»ºå¥½ï¼Œæ•°æ®å‡†å¤‡å®Œæ¯•ä¹‹åå°±å¯ä»¥å¼€å§‹è¿›è¡Œé­”æ€§çš„è®­ç»ƒäº†ã€‚ç›´æ¥æ‰§è¡Œtrain_ads.pyè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼š  
```shell
(yolov5) zhongming@ZhongMingdeMacBook-Pro yolov5 % python train_ads.py                                                   
train: weights=yolov5s.pt, cfg=, data=data/ads.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=300, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=True, adam=False, sync_bn=False, workers=8, project=runs/train, entity=None, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1, freeze=0, patience=30
github: âš ï¸ YOLOv5 is out of date by 25 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.
YOLOv5 ğŸš€ v5.0-405-gfad57c2 torch 1.9.0 CPU

hyperparameters: lr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0
TensorBoard: Start with 'tensorboard --logdir runs/train', view at http://localhost:6006/
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: 3
wandb: You chose 'Don't visualize my results'
wandb: WARNING `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 18h6dxo0.
wandb: W&B syncing is set to `offline` in this directory.  Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Overriding model.yaml nc=80 with nc=1

                 from  n    params  module                                  arguments                     
  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    
  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                
  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   
  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               
  4                -1  3    156928  models.common.C3                        [128, 128, 3]                 
  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              
  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 
  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              
  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        
  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          
 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              
 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 12           [-1, 6]  1         0  models.common.Concat                    [1]                           
 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          
 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              
 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          
 16           [-1, 4]  1         0  models.common.Concat                    [1]                           
 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          
 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              
 19          [-1, 14]  1         0  models.common.Concat                    [1]                           
 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          
 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              
 22          [-1, 10]  1         0  models.common.Concat                    [1]                           
 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          
 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]
[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.
Model Summary: 283 layers, 7063542 parameters, 7063542 gradients, 16.4 GFLOPs

Transferred 356/362 items from yolov5s.pt
Scaled weight_decay = 0.0005
optimizer: SGD with parameter groups 59 weight, 62 weight (no decay), 62 bias
train: Scanning 'data/train' images and labels...16 found, 0 missing, 0 empty, 0 corrupted: 100%|â–ˆâ–ˆ| 16/16 [00:02<00:00,  6.10it/s]
train: New cache created: data/train.cache
val: Scanning 'data/val' images and labels...2 found, 0 missing, 0 empty, 0 corrupted: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.46s/it]
val: New cache created: data/val.cache
Plotting labels... 

autoanchor: Analyzing anchors... anchors/target = 4.44, Best Possible Recall (BPR) = 1.0000
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/train/exp3
Starting training for 300 epochs...

     Epoch   gpu_mem       box       obj       cls    labels  img_size
     0/299        0G    0.1386   0.01956         0        28       640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.46s/it]
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.56it/s]                 all          2          0          0          0          0          0
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.56it/s]

     Epoch   gpu_mem       box       obj       cls    labels  img_size
     1/299        0G    0.1378    0.0202         0        31       640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:27<00:00, 27.41s/it]
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.66it/s]
                 all          2          0          0          0          0          0
     Epoch   gpu_mem       box       obj       cls    labels  img_size
   150/299        0G   0.05562   0.01635         0        27       640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:26<00:00, 26.94s/it]
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.85it/s]
                 all          2          2       0.99        0.5      0.535      0.252

     Epoch   gpu_mem       box       obj       cls    labels  img_size
   151/299        0G   0.05614   0.01598         0        23       640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:26<00:00, 26.93s/it]
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.86it/s]
                 all          2          2      0.997        0.5      0.538      0.207
EarlyStopping patience 30 exceeded, stopping training.

152 epochs completed in 8.084 hours.
Optimizer stripped from runs/train/exp3/weights/last.pt, 14.4MB
Optimizer stripped from runs/train/exp3/weights/best.pt, 14.4MB

wandb: Waiting for W&B process to finish, PID 63332
wandb: Program ended successfully.
wandb: Find user logs for this run at: /Users/zhongming/PycharmProjects/yolov5/wandb/offline-run-20210913_191626-18h6dxo0/logs/debug.log
wandb: Find internal logs for this run at: /Users/zhongming/PycharmProjects/yolov5/wandb/offline-run-20210913_191626-18h6dxo0/logs/debug-internal.log
wandb: Run summary:
wandb:                 train/box_loss 0.05614
wandb:                 train/obj_loss 0.01598
wandb:                 train/cls_loss 0.0
wandb:              metrics/precision 0.99749
wandb:                 metrics/recall 0.5
wandb:                metrics/mAP_0.5 0.53848
wandb:           metrics/mAP_0.5:0.95 0.20678
wandb:                   val/box_loss 0.06087
wandb:                   val/obj_loss 0.02391
wandb:                   val/cls_loss 0.0
wandb:                          x/lr0 0.0009
wandb:                          x/lr1 0.0009
wandb:                          x/lr2 0.0858
wandb:                       _runtime 29117
wandb:                     _timestamp 1631560903
wandb:                          _step 152
wandb: Run history:
wandb:         train/box_loss â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–…â–„â–„â–ƒâ–ƒâ–…â–„â–„â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚
wandb:         train/obj_loss â–…â–…â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–„â–„â–…â–„â–ƒâ–†â–„â–„â–…â–â–…â–…â–ƒâ–…â–†â–ˆâ–‡â–ˆâ–ˆâ–ƒâ–â–ƒâ–ƒâ–ƒâ–†â–„â–…â–‚â–†â–…â–„â–‚
wandb:         train/cls_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      metrics/precision â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–‚â–ƒâ–ƒâ–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         metrics/recall â–â–â–â–â–â–â–â–â–â–â–â–…â–…â–…â–â–…â–…â–ˆâ–…â–â–…â–…â–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…
wandb:        metrics/mAP_0.5 â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   metrics/mAP_0.5:0.95 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–…â–ˆâ–…â–†â–†â–…â–…â–†â–…â–…
wandb:           val/box_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–…â–…â–„â–„â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–
wandb:           val/obj_loss â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–„â–…â–‡â–†â–‡â–‡â–‡â–†â–‡â–†â–ˆâ–‡â–‡â–†â–†â–†â–„â–†
wandb:           val/cls_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                  x/lr0 â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  x/lr1 â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  x/lr2 â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:               _runtime â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:             _timestamp â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  _step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /Users/zhongming/PycharmProjects/yolov5/wandb/offline-run-20210913_191626-18h6dxo0

```
è®­ç»ƒç»“æŸä¹‹åçš„æ–‡ä»¶ä¿å­˜åœ¨Optimizer stripped from runs/train/exp3/weights/last.pt, 14.4MB  Optimizer stripped from runs/train/exp3/weights/best.pt, 14.4MBä¸¤ä¸ªæ–‡ä»¶ä¸­ã€‚åˆ°è¿™é‡Œæ¨¡å‹è®­ç»ƒå°±ç»“æŸäº†ã€‚  
åœ¨è®­ç»ƒä¸­ï¼Œä¹Ÿå¯ä»¥éšæ—¶æŸ¥çœ‹æ¯ä¸€è½®æ¬¡è®­ç»ƒçš„ç»“æœï¼Œå¯åˆ©ç”¨tensorboardå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œè®­ç»ƒå¼€å§‹æ—¶ä¼šåœ¨runs/train/expæ–‡ä»¶å¤¹ä¸­äº§ç”Ÿä¸€ä¸ªâ€œevents.out.tfevents.1608924773.JWX.5276.0â€æ–‡ä»¶ï¼Œåˆ©ç”¨tensorboardæ‰“å¼€å³å¯æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ã€‚é¦–å…ˆæˆ‘ä»¬é€šè¿‡cmdè¿›å»è¯¥YOLOv5æ‰€åœ¨çš„é¡¹ç›®æ–‡ä»¶å¤¹ï¼Œç„¶åæ¿€æ´»æ‰€ç”¨çš„è™šæ‹Ÿç¯å¢ƒï¼Œè¾“å…¥å¦‚ä¸‹å‘½ä»¤è¡Œï¼š
```shell
tensorboard --logdir runs/train/exp
```

å‘½ä»¤è¡Œè¾“å…¥ä¿¡æ¯çš„æ•´ä½“æ˜¾ç¤ºå¦‚ä¸‹æ‰€ç¤ºï¼š
```shell
activate yolov5

ensorboard --logdir runs/train/exp
TensorFlow installation not found - running with reduced feature set.
Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all
TensorBoard 2.4.0 at http://localhost:6006/ (Press CTRL+C to quit)

```
åˆ°è¿™ä¸€æ­¥åï¼Œæˆ‘ä»¬å°±å¯æ‰“å¼€ http://localhost:6006/ ç½‘é¡µæŸ¥çœ‹æ¯ä¸€è½®æ¬¡è®­ç»ƒçš„ç»“æœï¼Œå¦‚å›¾æ‰€ç¤ºã€‚
![board](atricle_images/board.png)  
**6.å®ç°æ£€æŸ¥**  
ä¿®æ”¹detect_ads.pyä¸­çš„å‚æ•°ä¸»è¦ç”¨åˆ°çš„åªæœ‰è¿™å‡ ä¸ªå‚æ•°ï¼šâ€“weightsï¼Œâ€“sourceï¼Œâ€“conf-thresï¼Œâ€“projectï¼š  
```python
def parse_opt():
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', nargs='+', type=str, default='runs/train/exp3/weights/last.pt', help='model.pt path(s)')
    parser.add_argument('--source', type=str, default='data/images', help='file/dir/URL/glob, 0 for webcam')
    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')
    parser.add_argument('--conf-thres', type=float, default=0.1, help='confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')
    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--view-img', action='store_true', help='show results')
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')
    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--visualize', action='store_true', help='visualize features')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--project', default='runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')
    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')
    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')
    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')
    opt = parser.parse_args()
    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand
    return opt


```
é€šè¿‡ä¸‹é¢çš„å‘½ä»¤è¿›è¡Œæ£€æµ‹ï¼š  
```shell
python detect_ads.py --source /Users/zhongming/PycharmProjects/skip_ads_yolo/Screenshots/Screenshot_20210517_135446_tv.danmaku.bili.jpg
```  
æ£€æµ‹å›æ˜¾ï¼š  
```shell
python detect_ads.py --source /Users/zhongming/PycharmProjects/skip_ads_yolo/Screenshots/Screenshot_20210517_135446_tv.danmaku.bili.jpg
detect: weights=runs/train/exp3/weights/best.pt, source=/Users/zhongming/PycharmProjects/skip_ads_yolo/Screenshots/Screenshot_20210517_135446_tv.danmaku.bili.jpg, imgsz=[640, 640], conf_thres=0.01, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False
YOLOv5 ğŸš€ v5.0-405-gfad57c2 torch 1.9.0 CPU

Fusing layers... 
[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.
Model Summary: 224 layers, 7053910 parameters, 0 gradients, 16.3 GFLOPs
image 1/1 /Users/zhongming/PycharmProjects/skip_ads_yolo/Screenshots/Screenshot_20210517_135446_tv.danmaku.bili.jpg: 640x320 31 skips, Done. (0.297s)
Results saved to runs/detect/exp22
Done. (0.388s)
```
å¦‚æœä¸ä¿®æ”¹detect.pyæ–‡ä»¶è¿™å¯ä»¥é€šè¿‡ä¸‹é¢çš„å‘½ä»¤è¿›è¡Œæ£€æµ‹ï¼š  
```shell
yolov5 % python detect_ads.py --source /Users/zhongming/PycharmProjects/skip_ads_yolo/Screenshots/Screenshot_20210517_135446_tv.danmaku.bili.jpg --weights  ads_best.pt 
```
æ£€æµ‹æ•ˆæœï¼Œç”±äºæ ·æœ¬å¤ªå°‘ï¼Œå¯¼è‡´æ£€æµ‹åˆ°çš„æ¦‚ç‡å¤ªä½ï¼Œä¸ºäº†æ˜¾ç¤ºå‡ºæ¥è¿™é‡ŒæŠŠparser.add_argument('--conf-thres', type=float, default=0.1,ï¼‰--conf-thresè°ƒæˆäº†0.1å¦åˆ™ç›´æ¥æ£€æµ‹ä¸åˆ°è·³è¿‡æŒ‰é’®ï¼Œè°ƒä½ä¹‹åæ£€æµ‹åˆ°çš„è·³è¿‡æŒ‰é’®æœ‰çš„å¹¶ä¸æ˜¯è·³è¿‡æŒ‰é’®(åä¸€å¼ å›¾)ã€‚  
![ok](atricle_images/18.jpg)  
ä¸‹å›¾æ£€æµ‹åˆ°çš„æŒ‰é’®æ˜¯æœ‰é—®é¢˜çš„ï¼š  
![error](atricle_images/19.jpg)  
ä¸ºäº†æé«˜æ£€æµ‹å‡†ç¡®åº¦ï¼Œä¸‹ä¸€æ­¥ä¼šåŠ å¤§æ ·æœ¬é‡å†æ¬¡è®­ç»ƒï¼Œç­‰å¾…çœ‹æ•ˆæœå§ ~~  

å®‰å“apkå¼€å±å¹¿å‘Šè·³è¿‡æŒ‰é’®è¯†åˆ«é¡¹ç›®  


---
obaby@mars  
http://www.h4ck.org.cn  
http://www.obaby.org.cn  


å‚è€ƒé“¾æ¥ï¼š  
https://github.com/EvgenMeshkin/ScreenShot/blob/master/app/src/main/java/by/android/evgen/screenshot/ScreenActivity.java  
https://blog.csdn.net/qq_38499859/article/details/90523283  
https://juejin.cn/post/6844903589127651335  
https://stackoverflow.com/questions/2661536/how-to-programmatically-take-a-screenshot-on-android  
https://pytorch.org  
https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exe_network
